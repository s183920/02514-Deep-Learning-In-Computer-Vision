{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oW29Y96P5LrX"
   },
   "source": [
    "# Exercise 1.3\n",
    "## Classification of MNIST digits with a convolutional neural network\n",
    "\n",
    "In this exercise we will classify MNIST digits again, but this time we will use a convolutional neural network (CNN).\n",
    "\n",
    "## Part 1: Using Jupyter notebook\n",
    "The exercise is written throughout this Jupyter notebook, and you should feel free to solve it within the notebook -- but you should also feel free to directly implement it as a script and run it in the terminal from the start (this will be part 2).\n",
    "\n",
    "First we import the modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jz2q4lHP5LrY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr5H-aka5Lrc"
   },
   "source": [
    "We check that this script has a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1Uvbi4IX5Lrc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h9w4bzfX5Lrh"
   },
   "source": [
    "We import the MNIST dataset, which is built into pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "yF0nU9c85Lri"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# trainset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "# train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "# testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "# test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# # data augmentation\n",
    "# augmented_trainset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "#     transforms.ToTensor()\n",
    "# ]))\n",
    "# train_loader = DataLoader(augmented_trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# augmented_testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "#     transforms.ToTensor()\n",
    "# ]))\n",
    "# test_loader = DataLoader(augmented_testset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def load_data(batch_size = 64, augmentations = False):\n",
    "    trainset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    if augmentations == True:\n",
    "        augmented_trainset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            # transforms.RandomPerspective(distortion_scale=0.9, p= 0.9),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        train_loader = DataLoader(augmented_trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "        testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "        test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size = 64, augmentations = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8PnRF_Ev5Lrm"
   },
   "source": [
    "You should implement a network to classify MNIST digits. \n",
    "The network should consist of two parts, a part with convolutions and one with fully connected layers.\n",
    "The convolutional part we will call `convolutional`, and it should contain the follwing:\n",
    "* two convolutional layers with 8 features\n",
    "* a $2\\times2$ max pooling layer\n",
    "* two convolutional layers with 16 features\n",
    "\n",
    "The convolutions should be $3\\times 3$, and should not change the size of the output. What does this mean that the stride and padding should be?\n",
    "\n",
    "For example check the documentation of the `nn` module https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "**Remember**: There's a specific type of layer that you should always have after a convolution or a fully connected layer. What is this type of layer called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "HqJTyYy35Lrn"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), # a convolutional layer with 8 features\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "           \n",
    "            )\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "                nn.Linear(14*14*16, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 10),\n",
    "                nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EKI3L0rh5Lrq"
   },
   "source": [
    "We instantiate a copy of our network, transfer it to the GPU if it's available.\n",
    "We also check if the dimensions of our network match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mD7N5AZA5Lrr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output from the convolutional part torch.Size([64, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#Get the first minibatch\n",
    "data = next(iter(train_loader))[0].cuda()\n",
    "#Try running the model on a minibatch\n",
    "print('Shape of the output from the convolutional part', model.convolutional(data).shape)\n",
    "model(data); #if this runs the model dimensions fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GCjfL-y_5Lru"
   },
   "source": [
    "We train this network for five epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "XyuQgHmE5Lrv",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b68777c47b84262ad2ca0c145fd8d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafe327e40f84a23b76bfc1e6eff23b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 73.4%\t test: 94.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079b1961bbee427c8a69bef122f87840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 89.3%\t test: 95.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7596311a13467280d3f550ff916d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 92.3%\t test: 96.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b762f20dad74e9ab5c47fa3f1b5ba36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 93.8%\t test: 97.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf34062964384f15a7fdd6b1bc72be46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 94.8%\t test: 98.1%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "    #For each epoch\n",
    "    train_correct = 0\n",
    "    for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #Zero the gradients computed for each weight\n",
    "        optimizer.zero_grad()\n",
    "        #Forward pass your image through the network\n",
    "        output = model(data)\n",
    "        #Compute the loss\n",
    "        loss = F.nll_loss(torch.log(output), target)\n",
    "        #Backward pass through the network\n",
    "        loss.backward()\n",
    "        #Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Compute how many were correctly classified\n",
    "        predicted = output.argmax(1)\n",
    "        train_correct += (target==predicted).sum().cpu().item()\n",
    "    #Comput the test accuracy\n",
    "    test_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        predicted = output.argmax(1).cpu()\n",
    "        test_correct += (target==predicted).sum().item()\n",
    "    train_acc = train_correct/len(trainset)\n",
    "    test_acc = test_correct/len(testset)\n",
    "    print(\"Accuracy train: {train:.1f}%\\t test: {test:.1f}%\".format(test=100*test_acc, train=100*train_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zoEC9oDH5Lr0"
   },
   "source": [
    "Hopefully you now have a model that's able to achieve decent performance on MNIST.\n",
    "It should have around 97.5% accuracy on the test set after the first epoch.\n",
    "\n",
    "* Why is the accuracy on the training set higher than on the test set? (recall from machine learning)\n",
    "\n",
    "* Why does it have higher accuracy on the test set than the training set after the first epoch?\n",
    "\n",
    "   hint: it's related to how the train accuracy is computed\n",
    "\n",
    "### Data augmentation\n",
    " * Add random rotations to the MNIST digits during training (you have to go back and modify the dataloader)\n",
    " \n",
    "  hint: you can use `transforms.RandomRotation` \n",
    "  \n",
    "  hint: you can combine multiple transforms into one with `transforms.Compose`\n",
    "\n",
    "How does this affect your training and testing loss?\n",
    "\n",
    " * Try plotting some of the augmented images, to visually confirm what your augmentation is doing.\n",
    "\n",
    " * Try adding another type of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     plt\u001b[39m.\u001b[39;49mimshow(data[i][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(), cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[39m.\u001b[39mtitle(data[i][\u001b[39m1\u001b[39m])\n\u001b[1;32m      7\u001b[0m     plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(A, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    696\u001b[0m     A \u001b[39m=\u001b[39m pil_to_array(A)  \u001b[39m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39;49msafe_masked_invalid(A, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m    701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:709\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_masked_invalid\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 709\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(x, subok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    710\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39misnative:\n\u001b[1;32m    711\u001b[0m         \u001b[39m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    712\u001b[0m         \u001b[39m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39mcopy)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Swap to native order.\u001b[39;00m\n",
      "File \u001b[0;32m~/torch/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(data[i][0].squeeze(), cmap='gray')\n",
    "    plt.title(data[i][1])\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the first 10 images in the testset and their predicted label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(testset[i][0].squeeze(), cmap='gray')\n",
    "    plt.title(testset[i][1])\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3Uf7eO8P5Lr1"
   },
   "source": [
    "### Explore the model\n",
    "What has the model learned? You can access all the weights in the model with `model.parameters()`. Here we just print the shape.\n",
    " - Try showing images of the filters in the first layer. \n",
    " - Can you from the dimensions of the weights alone identify which layer it is in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-lkTsfgo5Lr1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([8, 1, 3, 3]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8, 8, 3, 3]),\n",
       " torch.Size([8]),\n",
       " torch.Size([16, 8, 3, 3]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16, 16, 3, 3]),\n",
       " torch.Size([16]),\n",
       " torch.Size([500, 3136]),\n",
       " torch.Size([500]),\n",
       " torch.Size([10, 500]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.shape for w in model.parameters()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Fyc1SG5Lr4"
   },
   "source": [
    "### Dropout\n",
    " * Try adding dropout to your model.\n",
    " \n",
    "You can add it between the convolutional layers and or in the fully connected part.\n",
    "\n",
    "Remember to call `net.train()` and `net.eval()` to change the model from test to training state, so it knows when you want it to apply dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solving the exercise with a script to be run in the terminal\n",
    "Next, implement your code in a (reasonably clean) python script and run it from a terminal on HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
