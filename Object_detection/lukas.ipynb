{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datatype = \"train\", img_size = None):\n",
    "        self.datatype = datatype\n",
    "        self.root = '/dtu/datasets1/02514/data_wastedetection/'\n",
    "        self.anns_file_path = self.root + '/' + 'annotations.json'\n",
    "        self.coco = COCO(self.anns_file_path)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "\n",
    "        self.img_size = img_size\n",
    "\n",
    "\n",
    "        trns = []      \n",
    "        self.transforms = transforms.Compose([\n",
    "            # transforms.PILToTensor(),\n",
    "            *trns,\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.category_id_to_name = {d[\"id\"]: d[\"name\"] for d in self.coco.dataset[\"categories\"]}\n",
    "        \n",
    "        # split into train and test\n",
    "        np.random.seed(0)\n",
    "        idxs = np.arange(len(self.ids))\n",
    "        idxs = np.random.permutation(idxs)\n",
    "        self.train_idxs = idxs[:int(0.8*len(idxs))]\n",
    "        self.test_idxs = idxs[int(0.8*len(idxs)):]\n",
    "        self.train_idxs = self.train_idxs[:int(0.8*len(self.train_idxs))]\n",
    "        self.val_idxs = self.train_idxs[int(0.8*len(self.train_idxs)):]\n",
    "\n",
    "        print(f\"Number of train images: {len(self.train_idxs)}\")\n",
    "        print(f\"Number of val images: {len(self.val_idxs)}\")\n",
    "        print(f\"Number of test images: {len(self.test_idxs)}\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype == \"train\":\n",
    "            idx = self.train_idxs[idx]\n",
    "        elif self.datatype == \"val\":\n",
    "            idx = self.val_idxs[idx]\n",
    "        elif self.datatype == \"test\":\n",
    "            idx = self.test_idxs[idx]\n",
    "            \n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[idx]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # Resize image and calculate the scaling factor based on the aspect ratio\n",
    "        if self.img_size is not None:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_width / img_height \n",
    "            target_width = int(self.img_size[0]) \n",
    "            target_height = int(self.img_size[0] / aspect_ratio) if self.img_size is not None else img_height\n",
    "            if target_height > self.img_size[1]:\n",
    "                target_height = int(self.img_size[1])\n",
    "                target_width = int(self.img_size[1] * aspect_ratio)\n",
    "\n",
    "            img = img.resize((target_width, target_height), resample=Image.LANCZOS) # Image.ANTIALIAS)\n",
    "\n",
    "        \n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            if self.img_size is None:\n",
    "                xmin = coco_annotation[i]['bbox'][0]\n",
    "                ymin = coco_annotation[i]['bbox'][1]\n",
    "                xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "                ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                xmin = coco_annotation[i]['bbox'][0] * target_width / img_width\n",
    "                ymin = coco_annotation[i]['bbox'][1] * target_height / img_height\n",
    "                xmax = (coco_annotation[i]['bbox'][0] + coco_annotation[i]['bbox'][2]) * target_width / img_width\n",
    "                ymax = (coco_annotation[i]['bbox'][1] + coco_annotation[i]['bbox'][3]) * target_height / img_height\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        # labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        label_ids, labels = [], []\n",
    "        for i in range(num_objs):\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "            # label_ids.append(coco_annotation[i]['category_id'])\n",
    "            # labels.append(self.category_id_to_name[coco_annotation[i]['category_id']])\n",
    "        labels = torch.as_tensor(labels)\n",
    "        # label_ids = torch.as_tensor(label_ids, dtype=torch.int64)\n",
    "        \n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        \n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        # my_annotation[\"label_ids\"] = label_ids\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.ids)\n",
    "        if self.datatype == \"train\":\n",
    "            return len(self.train_idxs)\n",
    "        elif self.datatype == \"val\":\n",
    "            return len(self.val_idxs)\n",
    "        elif self.datatype == \"test\":\n",
    "            return len(self.test_idxs)\n",
    "\n",
    "def get_dataloader(dataset):\n",
    "    # collate_fn needs for batch\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # Batch size\n",
    "    train_batch_size = 1\n",
    "\n",
    "    # own DataLoader\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def show_img(img, annotations, label_dict, ax = None):\n",
    "    \"\"\"Show image with annotations\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    img = img.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    for idx in range(len(annotations[\"boxes\"])):\n",
    "        box = annotations[\"boxes\"][idx].cpu()\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # add label\n",
    "        label_id = annotations[\"labels\"][idx].cpu()\n",
    "        label = label_dict[label_id.item()]\n",
    "        ax.text(xmin, ymin, f\"{label}\", fontsize=12, color=\"r\")\n",
    "        \n",
    "        \n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of train images: 960\n",
      "Number of val images: 192\n",
      "Number of test images: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14076/4271965778.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((target_width, target_height), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_14076/4271965778.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((target_width, target_height), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_14076/4271965778.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((target_width, target_height), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_14076/4271965778.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((target_width, target_height), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 41.8990,  67.2837, 253.7740, 224.9279]], device='cuda:0'), 'labels': tensor([44], device='cuda:0'), 'image_id': tensor([465], device='cuda:0'), 'area': tensor([2632677.], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "# create own Dataset\n",
    "dataset = TacoDataset(img_size=(300, 300), datatype=\"train\")\n",
    "data_loader = get_dataloader(dataset)\n",
    "\n",
    "\n",
    "# DataLoader is iterable over Dataset\n",
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    \n",
    "    show_img(imgs[0], annotations[0], dataset.category_id_to_name)\n",
    "    plt.savefig(\"test.png\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14076/4271965778.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((target_width, target_height), Image.ANTIALIAS)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 225\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m img_array \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m#.transpose(1, 2, 0)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# img_cv2 = cv2.imdecode(img.cpu().numpy().transpose(1, 2, 0), cv2.IMREAD_COLOR)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m img_cv2 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img_array, cv2\u001b[39m.\u001b[39;49mCOLOR_RGB2BGR)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Selective search\u001b[39;00m\n\u001b[1;32m     14\u001b[0m SS \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mximgproc\u001b[39m.\u001b[39msegmentation\u001b[39m.\u001b[39mcreateSelectiveSearchSegmentation()\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 225\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Load image\n",
    "img, annotations = dataset[0]\n",
    "\n",
    "# Convert to cv2\n",
    "img_array = img.cpu().numpy()#.transpose(1, 2, 0)\n",
    "# img_cv2 = cv2.imdecode(img.cpu().numpy().transpose(1, 2, 0), cv2.IMREAD_COLOR)\n",
    "img_cv2 = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "# Selective search\n",
    "SS = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "SS.setBaseImage(img_cv2)\n",
    "SS.switchToSelectiveSearchFast()\n",
    "results = SS.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3120, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[1467.1702, 1496.3195, 2162.1702, 2492.6528]]),\n",
       " 'labels': tensor([50]),\n",
       " 'image_id': tensor([471]),\n",
       " 'area': tensor([368758.4375])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLCI-venv",
   "language": "python",
   "name": "dlci-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
